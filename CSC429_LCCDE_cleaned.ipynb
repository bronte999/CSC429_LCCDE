{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJP6_Oj1CsZR"
      },
      "source": [
        "This is the code adapted from the paper entitled \"LCCDE: A Decision-Based Ensemble Framework for Intrusion Detection in The Internet of Vehicles\" accepted in 2022 IEEE Global Communications Conference (GLOBECOM).\n",
        "Authors: Li Yang (lyang339@uwo.ca), Abdallah Shami (Abdallah.Shami@uwo.ca), Gary Stevens, and Stephen de Rusett\n",
        "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University, Ontario, Canada; S2E Technologies, St. Jacobs, Ontario, Canada\n",
        "\n",
        "L. Yang, A. Shami, G. Stevens, and S. DeRusett, â€œLCCDE: A Decision-Based Ensemble Framework for Intrusion Detection in The Internet of Vehicles,\" in 2022 IEEE Global Communications Conference (GLOBECOM), 2022, pp. 1-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx4Ho29bAc8g"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install catboost\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pR8ZX0YAHdb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import lightgbm as lgb\n",
        "import catboost as cbt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "from river import stream\n",
        "from statistics import mode\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfeJWj9-ADw0"
      },
      "outputs": [],
      "source": [
        "# be careful of ignoring warnings\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive to save models later\n",
        "import gc\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Z7Qb5qJ_S3n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_next_int():\n",
        "  i = 0\n",
        "  while True:\n",
        "    yield i\n",
        "    i += 1\n",
        "\n",
        "g = return_next_int()\n",
        "\n",
        "\n",
        "# settings for dataset mode\n",
        "MODE_CAR_HACKING = next(g)  # use the Car-Hacking dataset\n",
        "MODE_ROAD = next(g)  # use the ROAD dataset\n",
        "MODE_TRAIN_CAR_HACKING_TEST_ROAD = next(g)  # train on the Car-Hacking dataset and test on the ROAD dataset\n",
        "MODE_CAR_HACKING_BINARY = next(g)  # change Car-Hacking labels to two classes (to compare with MODE_TRAIN_CAR_HACKING_TEST_ROAD)"
      ],
      "metadata": {
        "id": "QJrNWqcFotUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set mode\n",
        "dataset_mode = MODE_CAR_HACKING\n",
        "\n",
        "def dataset_mode_error(invalid_dataset_mode: int):\n",
        "  raise ValueError(\"Unsupported dataset mode: {m}\".format(m=invalid_dataset_mode))"
      ],
      "metadata": {
        "id": "cFfeKrSIuili"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paths for datasets\n",
        "PATH_CAR_HACKING = \"/content/drive/MyDrive/car_hacking_with_header.csv\"\n",
        "PATH_ROAD = \"/content/drive/MyDrive/road.csv\""
      ],
      "metadata": {
        "id": "dztsu0QusyJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LCCDE_Model:\n",
        "  \"\"\"Basically a struct containing data for a base learner in the LCCDE.\"\"\"\n",
        "  def __init__(self, model, name):\n",
        "    # base learner object that should have the predict() and predict_proba() functions (e.g. XGBClassifier())\n",
        "    self.model = model\n",
        "    # string name\n",
        "    self.name = name\n",
        "    # the following attributes are for evaluation metrics\n",
        "    self.accuracy = None\n",
        "    self.precision = None\n",
        "    self.recall = None\n",
        "    self.f1_avg = None  # average of F1 scores\n",
        "    self.f1 = None  # list of F1 scores for each class\n",
        "    # the following attributes are for storing predictions for one data point\n",
        "    self.predicted_class = None  # predicted class\n",
        "    self.highest_predicted_prob = None  # class with highest confidence score\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"LCCDE_Model({m}, {n})\".format(m=self.model, n=self.name)\n",
        "\n",
        "  def store_eval_metrics(self, y_test, y_pred):\n",
        "    self.accuracy = accuracy_score(y_test, y_pred)\n",
        "    self.precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    self.recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    self.f1_avg = f1_score(y_test, y_pred, average='weighted')\n",
        "    self.f1 = f1_score(y_test, y_pred, average=None)"
      ],
      "metadata": {
        "id": "lZr50NHFyJ3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select base learners to use\n",
        "USE_LG = True  # LightGBM\n",
        "USE_XG = True  # XGBoost\n",
        "USE_ADA = True  # AdaBoost\n",
        "USE_CB = True  # CatBoost"
      ],
      "metadata": {
        "id": "4XTOTrIquxS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the models that we're using and wrap them in LCCDE_Model class\n",
        "ensemble_models = []\n",
        "\n",
        "if USE_LG:\n",
        "  import lightgbm as lgb\n",
        "  lg = LCCDE_Model(lgb.LGBMClassifier(), \"LightGBM\")\n",
        "  ensemble_models.append(lg)\n",
        "if USE_XG:\n",
        "  import xgboost as xgb\n",
        "  xg = LCCDE_Model(xgb.XGBClassifier(), \"XGBoost\")\n",
        "  ensemble_models.append(xg)\n",
        "if USE_ADA:\n",
        "  from sklearn.ensemble import AdaBoostClassifier\n",
        "  ada = LCCDE_Model(AdaBoostClassifier(), \"AdaBoost\")\n",
        "  ensemble_models.append(ada)\n",
        "if USE_CB:\n",
        "  import catboost as cbt\n",
        "  cb = LCCDE_Model(cbt.CatBoostClassifier(verbose=True,boosting_type='Plain'), \"CatBoost\")\n",
        "  ensemble_models.append(cb)"
      ],
      "metadata": {
        "id": "7As9t57NyMMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrUFJlOaAnkk"
      },
      "outputs": [],
      "source": [
        "# read files into dataframes\n",
        "\n",
        "dtype_dict = {\"Time\": float, \"Id\": str,\n",
        "              \"Byte1\": str, \"Byte2\": str, \"Byte3\": str, \"Byte4\": str,\n",
        "              \"Byte5\": str, \"Byte6\": str, \"Byte7\": str, \"Byte8\": str,\n",
        "              \"Label\": str}  # maps column names to their types\n",
        "\n",
        "\n",
        "def read_car_hacking() -> pd.DataFrame:\n",
        "  return pd.read_csv(PATH_CAR_HACKING, on_bad_lines=\"warn\", dtype=dtype_dict)\n",
        "\n",
        "def read_road() -> pd.DataFrame:\n",
        "  return pd.read_csv(PATH_ROAD, on_bad_lines=\"warn\", dtype=dtype_dict)\n",
        "\n",
        "\n",
        "if dataset_mode == MODE_CAR_HACKING or dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  df = read_car_hacking()\n",
        "\n",
        "elif dataset_mode == MODE_ROAD:\n",
        "  df = read_road()\n",
        "\n",
        "elif dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD:\n",
        "  df = read_car_hacking()\n",
        "  df_test = read_road()\n",
        "\n",
        "else:\n",
        "  dataset_mode_error(dataset_mode)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for transferring between datasets, turn it into a binary classification problem\n",
        "\n",
        "def make_non_benign_malicious(data_frame):\n",
        "  \"\"\"set all Labels that aren't 'Benign' to 'Malicious' (mutates the data frame)\"\"\"\n",
        "  label_col = \"Label\"\n",
        "  benign_label = \"Benign\"\n",
        "  malicious_label = \"Malicious\"\n",
        "  data_frame.loc[data_frame[label_col] != benign_label, [label_col]] = malicious_label\n",
        "\n",
        "\n",
        "if dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD or dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  make_non_benign_malicious(df)  # change labels for Car-Hacking dataframe\n",
        "  print(\"Changed labels for Car-Hacking dataframe\")\n",
        "  if dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD:\n",
        "    make_non_benign_malicious(df_test)  # change labels for ROAD dataframe\n",
        "    print(\"Changed labels for ROAD dataframe\")"
      ],
      "metadata": {
        "id": "2rPbUJjfskZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIm4fqRvLdbg"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"].unique()"
      ],
      "metadata": {
        "id": "DdzLTxl9Q08N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ETNEB4UOT5u"
      },
      "outputs": [],
      "source": [
        "if dataset_mode == MODE_CAR_HACKING or dataset_mode == MODE_ROAD or dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  # encode labels\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "  label_encoder_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "elif dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD:\n",
        "  # combine dataframes because they have the same columns\n",
        "  df_len = len(df)\n",
        "  temp = pd.concat([df, df_test], ignore_index=True)\n",
        "  # encode labels\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  temp['Label'] = label_encoder.fit_transform(temp['Label'])\n",
        "  label_encoder_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "  # separate back into train and test dataframes\n",
        "  df = temp.iloc[:df_len].reset_index(drop=True)\n",
        "  df_test = temp.iloc[df_len:].reset_index(drop=True)\n",
        "\n",
        "else:\n",
        "  dataset_mode_error(dataset_mode)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "MzUttS0UaEd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder_name_mapping"
      ],
      "metadata": {
        "id": "NYpOV-JgUsJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNybtUHAKVV"
      },
      "outputs": [],
      "source": [
        "df.Label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "uWUKdEMEJ-2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop NA values from Car-Hacking\n",
        "if dataset_mode == MODE_CAR_HACKING or dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD or dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  count = df.isna().any(axis=0).sum()\n",
        "  print(\"Dropping {c} NA values...\".format(c=count))\n",
        "  df.dropna(inplace=True)\n",
        "  print(\"Done\")"
      ],
      "metadata": {
        "id": "qZLeEgYkE_EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbsXMLzpAs9b"
      },
      "outputs": [],
      "source": [
        "def get_X_and_y(data_frame: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"Prepares X (feature matrix) and y (the corresponding ground truth labels).\"\"\"\n",
        "  X = data_frame.drop(['Label','Time','Id'],axis=1)\n",
        "  y = data_frame['Label']\n",
        "  # convert object dtypes to ints\n",
        "  for col in X.columns:\n",
        "    if X[col].dtype == object:  # Check if the column type is object (likely non-numeric)\n",
        "        X[col] = X[col].apply(lambda x: int(x, 16))\n",
        "  return X, y\n",
        "\n",
        "\n",
        "if dataset_mode == MODE_CAR_HACKING or dataset_mode == MODE_ROAD or dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  # take train-test split of df\n",
        "  X, y = get_X_and_y(df)\n",
        "  # due to resource limitations, we've set the test size to 4% (train size will be its complement, 96%)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.04, random_state = 0, stratify=df[\"Label\"]) # shuffle=False\n",
        "\n",
        "elif dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD:\n",
        "  # use the entire Car-Hacking dataset for training\n",
        "  X_train, y_train = get_X_and_y(df)\n",
        "  # use the ROAD dataset for testing\n",
        "  # due to resource limitations, we've set the test size to 4% (train size will be its complement, 96%)\n",
        "  X, y = get_X_and_y(df_test)\n",
        "  _, X_test, _, y_test = train_test_split(X, y, test_size = 0.04, random_state = 0, stratify=df_test[\"Label\"]) # shuffle=False\n",
        "\n",
        "else:\n",
        "  dataset_mode_error(dataset_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cts27qvMO_QV"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "C54DLgBXrKFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "Iq4vS9Y-rK-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "12KHpJMarMC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.dtypes"
      ],
      "metadata": {
        "id": "da2QpW9ALcZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy14BoWvAvUs"
      },
      "outputs": [],
      "source": [
        "pd.Series(y_train).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_test).value_counts()"
      ],
      "metadata": {
        "id": "QT0L933g33nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqlK0JB7Awww"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "smote = None\n",
        "\n",
        "if dataset_mode == MODE_CAR_HACKING:\n",
        "  # about 14 million Benign and 472k to 628k of other classes, we aren't oversampling here to save time\n",
        "  pass\n",
        "\n",
        "elif dataset_mode == MODE_ROAD:\n",
        "  smote = SMOTE(sampling_strategy={2: 40000, 4: 40000, 3: 10000})\n",
        "\n",
        "elif dataset_mode == MODE_TRAIN_CAR_HACKING_TEST_ROAD:\n",
        "  # about 15 million Benign and 2 million Malicious, we aren't oversampling here to save time\n",
        "  pass\n",
        "\n",
        "elif dataset_mode == MODE_CAR_HACKING_BINARY:\n",
        "  # same as MODE_CAR_HACKING\n",
        "  pass\n",
        "\n",
        "else:\n",
        "  dataset_mode_error(dataset_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLiy3-9kA60W"
      },
      "outputs": [],
      "source": [
        "if smote is not None:\n",
        "  X_train, y_train = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iS-tdioA92n"
      },
      "outputs": [],
      "source": [
        "pd.Series(y_train).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_test).value_counts()"
      ],
      "metadata": {
        "id": "FcVUjsr1V3Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, y_test=None, y_pred=None):\n",
        "  if y_test is not None and y_pred is not None:\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "  print(\"Accuracy of {m}: {e}\".format(m=model.name, e=str(model.accuracy)))\n",
        "  print(\"Precision of {m}: {e}\".format(m=model.name, e=str(model.precision)))\n",
        "  print(\"Recall of {m}: {e}\".format(m=model.name, e=str(model.recall)))\n",
        "  print(\"Average F1 of {m}: {e}\".format(m=model.name, e=str(model.f1_avg)))\n",
        "  print(\"F1 of {m} for each type of attack: {e}\".format(m=model.name, e=str(model.f1)))\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  if y_test is not None and y_pred is not None:\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    f, ax = plt.subplots(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, linewidth=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "    plt.xlabel(\"y_pred\")\n",
        "    plt.ylabel(\"y_true\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DcArtNX82Do5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17QSeMDFA_wk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if USE_LG:\n",
        "  # Train the LightGBM algorithm\n",
        "  lg.model.fit(X_train, y_train)\n",
        "  y_pred_lg = lg.model.predict(X_test)\n",
        "\n",
        "  lg.store_eval_metrics(y_test, y_pred_lg)\n",
        "  evaluate_model(lg, y_test, y_pred_lg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCQ_NKAgBGdl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if USE_XG:\n",
        "  # Train the XGBoost algorithm\n",
        "  X_train_x = X_train.values\n",
        "  X_test_x = X_test.values\n",
        "  xg.model.fit(X_train_x, y_train)\n",
        "  y_pred_xg = xg.model.predict(X_test_x)\n",
        "\n",
        "  xg.store_eval_metrics(y_test, y_pred_xg)\n",
        "  evaluate_model(xg, y_test, y_pred_xg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if USE_ADA:\n",
        "  # Train the AdaBoost algorithm\n",
        "  ada.model.fit(X_train.values, y_train)\n",
        "  y_pred_ada = ada.model.predict(X_test)\n",
        "\n",
        "  ada.store_eval_metrics(y_test, y_pred_ada)\n",
        "  evaluate_model(ada, y_test, y_pred_ada)"
      ],
      "metadata": {
        "id": "Iyf6UhEh3ups"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfyavRrBIgc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if USE_CB:\n",
        "  # Train the CatBoost algorithm\n",
        "  cb.model.fit(X_train, y_train)\n",
        "  y_pred_cb = cb.model.predict(X_test)\n",
        "\n",
        "  cb.store_eval_metrics(y_test, y_pred_cb)\n",
        "  evaluate_model(cb, y_test, y_pred_cb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipru6NBXCgXG"
      },
      "outputs": [],
      "source": [
        "f1_scores = []  # list of f1 score lists for each model\n",
        "for m in ensemble_models:\n",
        "  f1_scores.append(m.f1)\n",
        "\n",
        "\n",
        "leader_models = dict()  # maps each Label (an int) to its leader model, the model with the highest\n",
        "\n",
        "# iterate through f1 scores (all f1_scores elements should be lists of the same length because all models were used on the same labels)\n",
        "for i in range(len(f1_scores[0])):  # for each class... (corresponding to indices in an element of f1_scores)\n",
        "  # find the model with the highest f1 for that class and add to leader_models dictionary\n",
        "  for m in ensemble_models:\n",
        "      max_f1 = max([f1[i] for f1 in f1_scores])\n",
        "      if max_f1 == m.f1[i]:\n",
        "          leader_models[i] = m.model\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R94t3XUVCiTf"
      },
      "outputs": [],
      "source": [
        "leader_models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LCCDE helper functions\n",
        "from itertools import groupby\n",
        "\n",
        "\n",
        "def all_equal(iterable):\n",
        "    \"\"\"Returns whether all values in an Iterable are equal\n",
        "    (https://stackoverflow.com/a/3844832).\n",
        "    \"\"\"\n",
        "    g = groupby(iterable)\n",
        "    return next(g, True) and not next(g, False)\n",
        "\n",
        "\n",
        "def all_unique(lst):\n",
        "    \"\"\"Returns whether all values in an Iterable are unique\n",
        "    (https://www.geeksforgeeks.org/python-check-if-list-contains-all-unique-elements/).\n",
        "    \"\"\"\n",
        "    # use the unique function from numpy to find the unique elements in the list\n",
        "    unique_elements, counts = np.unique(lst, return_counts=True)\n",
        "    # return True if all elements in the list are unique (i.e., the counts are all 1)\n",
        "    return all(counts == 1)"
      ],
      "metadata": {
        "id": "U3aS7SNN4m2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell defines a helper function LCCDE_predict_class(), and the ensemble prediction function LCCDE()\n",
        "from statistics import mode\n",
        "\n",
        "\n",
        "def LCCDE_predict_class(xi, models: list[LCCDE_Model], leader_models: dict):\n",
        "    \"\"\"Classifies a data record using the LCCDE model.\n",
        "    Returns the model's predicted class.\n",
        "    :param xi: features for a data record\n",
        "    :param models: list of LCCDE_Model objects\n",
        "    :param leader_models: a dictionary with Labels as keys, the value of each key should be m.model for any m in in models\n",
        "    \"\"\"\n",
        "    if all_equal([m.predicted_class for m in models]):\n",
        "        # if all models predict the same class, use that as final predicted class\n",
        "        final_pred_class = models[0].predicted_class\n",
        "\n",
        "    elif all_unique([m.predicted_class for m in models]):\n",
        "        # if all models predict a different class, choose final predicted class based on class leaders\n",
        "\n",
        "        # find models that are the leader for their predicted class\n",
        "        matching_models = []\n",
        "        for m in models:\n",
        "            if leader_models[m.predicted_class] == m.model:\n",
        "                matching_models.append(m)\n",
        "        if len(matching_models) == 1:\n",
        "            # if only one model is the leader for its predicted class, then use its prediction\n",
        "            final_pred_class = matching_models[0].predicted_class\n",
        "        else:\n",
        "            # otherwise, use the prediction of the model with highest confidence\n",
        "            highest_confidence = max([m.highest_predicted_prob for m in models])\n",
        "            most_confident_models = [m for m in models if m.highest_predicted_prob == highest_confidence]\n",
        "            final_pred_class = most_confident_models[0].predicted_class  # if there's a tie, just pick the first one\n",
        "\n",
        "    else:\n",
        "        # if some models agree and some don't, use the leader of the majority class as the final predicted class\n",
        "        majority_class = mode([m.predicted_class for m in models])  # if there's a tie, mode() will pick the first one\n",
        "        leader = leader_models[majority_class]\n",
        "        final_pred_class = leader.predict(xi)[0]\n",
        "\n",
        "    return final_pred_class\n",
        "\n",
        "\n",
        "def LCCDE(X_test, y_test, models: list[LCCDE_Model], leader_models, verbose=False) -> tuple[list, list]:\n",
        "    \"\"\"Uses the Leader Class and Confidence Decision Ensemble (LCCDE) to\n",
        "    classify records in a feature matrix. Casts predicted labels to ints.\n",
        "    Returns a tuple containing ground truth labels and predicted labels.\n",
        "    :param X_test: feature matrix for testing (pandas DataFrame)\n",
        "    :param y_test: ground truth Labels corresponding to X_test\n",
        "    :param models: a list of the LCCDE_Model objects\n",
        "    :param leader_models: a dictionary with Labels as keys, the value of each key should be a model in base_learners\n",
        "    \"\"\"\n",
        "    y_actual = []  # list of actual y-values (I think it ends up being the same as the y_test parameter)\n",
        "    y_predicted = []  # the values predicted by the ensemble for each xi in X_test\n",
        "\n",
        "    count = 0\n",
        "    # predict each label based on the features\n",
        "    for xi, yi in stream.iter_pandas(X_test, y_test):\n",
        "        xi = np.array(list(xi.values())).reshape(1, -1)\n",
        "\n",
        "        # for each model, predict class based on feature values xi\n",
        "        for m in models:\n",
        "            m.predicted_class = int(m.model.predict(xi)[0])  # predicted class for this data point xi\n",
        "            predicted_probs = m.model.predict_proba(xi)  # prediction probability confidence list\n",
        "            m.highest_predicted_prob = np.max(predicted_probs)  # max of prediction probability confidence list\n",
        "\n",
        "        # use the ensemble to predict the class of xi\n",
        "        final_pred_class = int(LCCDE_predict_class(xi, models, leader_models))\n",
        "\n",
        "        # save the actual and predicted y-values\n",
        "        y_actual.append(yi)\n",
        "        y_predicted.append(final_pred_class)\n",
        "\n",
        "        count += 1\n",
        "        if verbose and count % 1000 == 0:\n",
        "          print(\"Progress update: LCCDE has predicted {n} values\".format(n=count))\n",
        "\n",
        "    return y_actual, y_predicted"
      ],
      "metadata": {
        "id": "LN1P5Xs-afhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "6iKgpII4pI0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_models"
      ],
      "metadata": {
        "id": "9KS09bjFbkAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkhCg1KiCuNO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# run LCCDE() to predict classes for test set\n",
        "verbose = True\n",
        "y_actual, y_predicted = LCCDE(X_test, y_test, ensemble_models, leader_models, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92nbbXgbEIgC"
      },
      "outputs": [],
      "source": [
        "# The performance of the proposed LCCDE model\n",
        "lccde_ensemble = LCCDE_Model(None, \"LCCDE\")\n",
        "lccde_ensemble.store_eval_metrics(y_actual, y_predicted)\n",
        "evaluate_model(lccde_ensemble)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVgXZZJzEKJR"
      },
      "outputs": [],
      "source": [
        "# comparison of F1 scores\n",
        "for model in ensemble_models + [lccde_ensemble]:\n",
        "  print(\"F1 of {m} for each type of attack: {f}\".format(m=model.name, f=model.f1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "models_to_save = dict()  # maps model objects to file name to save them in\n",
        "if USE_LG:\n",
        "  models_to_save[lg] = \"lg.sav\"\n",
        "if USE_XG:\n",
        "  models_to_save[xg] = \"xg.sav\"\n",
        "if USE_ADA:\n",
        "  models_to_save[ada] = \"ada.sav\"\n",
        "\n",
        "# save to the root of your Google Drive (MyDrive)\n",
        "drive_path_prefix = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# save the models\n",
        "for m in models_to_save.keys():\n",
        "  pickle_file = open(drive_path_prefix + models_to_save[m], \"wb+\")\n",
        "  pickle.dump(m.model, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  pickle_file.close()"
      ],
      "metadata": {
        "id": "Cv-U1SR9PYr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_CB:\n",
        "  cb.model.save_model(\"CatboostModel\")"
      ],
      "metadata": {
        "id": "wZnoQJsA52A2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}